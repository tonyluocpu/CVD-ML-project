{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MaxAbsScaler, Normalizer, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correlated_pairs_amount(df: pd.DataFrame, threshold=0.9)->int:\n",
    "    \"\"\"\n",
    "    Finds the number of highly correlated column pairs in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pd.DataFrame\n",
    "        The input DataFrame to analyze.\n",
    "    - threshold: float\n",
    "        The correlation threshold above which columns are considered highly correlated.\n",
    "        \n",
    "    Returns:\n",
    "    - int\n",
    "        The number of column pairs with a correlation greater than the threshold, this pairs may be a huge number because it double count many \n",
    "        individual ones. \n",
    "    \"\"\"\n",
    "    # Compute the correlation matrix\n",
    "    corr_matrix = df.corr()\n",
    "    \n",
    "    # Mask the upper triangle of the correlation matrix\n",
    "    mask = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    upper_triangle = corr_matrix.where(mask)\n",
    "    \n",
    "    # Find column pairs with correlation above the threshold\n",
    "    correlated_pairs = [\n",
    "        (col1, col2) \n",
    "        for col1 in upper_triangle.columns \n",
    "        for col2 in upper_triangle.index \n",
    "        if abs(upper_triangle.loc[col2, col1]) > threshold\n",
    "    ]\n",
    "    \n",
    "    return len(correlated_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_correlated_groups(df: pd.DataFrame, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Finds the unique groups of highly correlated columns in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pd.DataFrame\n",
    "        The input DataFrame to analyze.\n",
    "    - threshold: float\n",
    "        The correlation threshold above which columns are considered highly correlated.\n",
    "        \n",
    "    Returns:\n",
    "    - int\n",
    "        The number of unique groups of correlated columns.\n",
    "    - list\n",
    "        A list of unique correlated groups.\n",
    "    \"\"\"\n",
    "    # Compute the correlation matrix\n",
    "    corr_matrix = df.corr()\n",
    "    \n",
    "    # Identify pairs of columns with correlation above the threshold\n",
    "    correlated_pairs = [\n",
    "        (col1, col2) \n",
    "        for col1 in corr_matrix.columns \n",
    "        for col2 in corr_matrix.columns \n",
    "        if col1 != col2 and abs(corr_matrix.loc[col1, col2]) > threshold\n",
    "    ]\n",
    "    \n",
    "    # Build a graph where nodes are columns and edges indicate high correlation\n",
    "    graph = nx.Graph()\n",
    "    graph.add_edges_from(correlated_pairs)\n",
    "    \n",
    "    # Find connected components in the graph (unique groups of correlated columns)\n",
    "    correlated_groups = list(nx.connected_components(graph))\n",
    "    \n",
    "    return len(correlated_groups), correlated_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_null_all_data(df: pd.DataFrame)->int:\n",
    "    '''\n",
    "    find the number of nulls across all dataset\n",
    "    - df: the dataframe\n",
    "    '''\n",
    "    return df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_null_cols(df: pd.DataFrame, str_lst: list[str]):\n",
    "    '''\n",
    "    find the number of nulls of some columns\n",
    "    - df: the dataframe\n",
    "    - str_lst: input the column names in list of string\n",
    "    '''\n",
    "    return df[str_lst].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_num_and_non_num(df: pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Separate the dataset into two DataFrames:\n",
    "    one with numeric columns (others filled with NaN)\n",
    "    and one with non-numeric columns (others filled with NaN).\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two DataFrames - numeric and non-numeric, preserving column names.\n",
    "    \"\"\"\n",
    "    df_numeric = df.apply(lambda col: col if pd.api.types.is_numeric_dtype(col) else pd.NA)\n",
    "    df_non_numeric = df.apply(lambda col: col if not pd.api.types.is_numeric_dtype(col) else pd.NA)\n",
    "    return df_numeric, df_non_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_correlated_columns(df: pd.DataFrame, threshold=0.6)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drops columns from groups of highly correlated columns until only one remains per group.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pd.DataFrame\n",
    "        The input DataFrame to analyze and modify.\n",
    "    - threshold: float\n",
    "        The correlation threshold above which columns are considered highly correlated.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame\n",
    "        A modified DataFrame with reduced columns.\n",
    "    \"\"\"\n",
    "    corr_matrix = df.corr()\n",
    "    \n",
    "    correlated_pairs = [\n",
    "        (col1, col2) \n",
    "        for col1 in corr_matrix.columns \n",
    "        for col2 in corr_matrix.columns \n",
    "        if col1 != col2 and abs(corr_matrix.loc[col1, col2]) > threshold\n",
    "    ]\n",
    "\n",
    "    graph = nx.Graph()\n",
    "    graph.add_edges_from(correlated_pairs)\n",
    "    correlated_groups = list(nx.connected_components(graph))\n",
    "    columns_to_keep = {list(group)[0] for group in correlated_groups}\n",
    "    uncorrelated_columns = set(df.columns) - set(graph.nodes)\n",
    "    columns_to_keep.update(uncorrelated_columns)  \n",
    "    return df[list(columns_to_keep)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_columns(df: pd.DataFrame, word_lst:list[str])->pd.DataFrame:\n",
    "    '''\n",
    "    Exclude all columns in the dataset if they contain specific words\n",
    "    - df: the dataframe\n",
    "    - word_lst: list of words to exclude if they are found in any column names\n",
    "    '''\n",
    "    df_new:pd.DataFrame = df.loc[:, ~df.columns.str.contains('|'.join(word_lst), case = False)]\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_single_columns_imputation(df: pd.DataFrame, exclude_columns: list[str] = None, n_neighbors: int = 5)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs KNN on numeric columns in the DataFrame to handle missing values or other tasks, skipping non-numeric data.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    exclude_columns (list[str], optional): A list of column names to exclude from KNN. Default is None.\n",
    "    n_neighbors (int): Number of neighbors for KNN. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with numeric columns updated using KNN.\n",
    "    \"\"\"\n",
    "    exclude_columns = exclude_columns or []\n",
    "    numeric_columns = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in exclude_columns]\n",
    "    if not numeric_columns:\n",
    "        return df\n",
    "    imputer  = KNNImputer(n_neighbors=n_neighbors)\n",
    "    df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer, MaxAbsScaler\n",
    "import numpy as np\n",
    "\n",
    "def standarize_dataframe(df: pd.DataFrame, exclude_columns: list[str] = None, method: str = 'z-score', norm: str = None)->pd.DataFrame:\n",
    "    '''\n",
    "    Standardizes or normalizes numeric columns in a DataFrame, with additional support for max-abs scaling and log transformation.\n",
    "\n",
    "    This function provides multiple methods for scaling or normalizing numeric data:\n",
    "\n",
    "    - **z-score**: Standardizes data by centering it around the mean (0) and scaling it to unit variance (standard deviation of 1). Best for data assumed to be normally distributed or needed for algorithms sensitive to variances (e.g., PCA, linear regression).\n",
    "    - **min-max**: Scales data to a specified range, usually [0, 1]. Ideal for feature scaling in distance-based algorithms like k-NN or neural networks where bounded ranges are helpful.\n",
    "    - **robust**: Scales data using the median and interquartile range, reducing the influence of outliers. Useful when data contains significant outliers.\n",
    "    - **max-abs**: Scales data by dividing each value by the maximum absolute value. Suitable for sparse data to preserve zero entries.\n",
    "    - **log**: Applies a logarithmic transformation (using log1p to handle zero values). Effective for compressing large value ranges and handling skewed data.\n",
    "\n",
    "    Additionally, normalization (L1 or L2) is available for row-wise scaling:\n",
    "\n",
    "    - **L1**: Scales rows so the sum of absolute values equals 1. Useful for sparse data or when focusing on proportions.\n",
    "    - **L2**: Scales rows so the Euclidean norm (square root of sum of squared values) equals 1. Often used in machine learning models sensitive to vector magnitude, like SVMs.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    exclude_columns (list[str]): List of column names to exclude. Default is None.\n",
    "    method (str): Standardization method ('z-score', 'min-max', 'robust', 'max-abs', 'log'). Default is 'z-score'.\n",
    "    norm (str): Normalization method ('l1', 'l2'). Default is None.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with standardized or normalized columns.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If an unsupported method or normalization type is provided.\n",
    "    '''\n",
    "    if exclude_columns is None:\n",
    "        exclude_columns = []\n",
    "\n",
    "    cols_to_standarize = [\n",
    "        col for col in df.select_dtypes(include='number').columns if col not in exclude_columns\n",
    "    ]\n",
    "\n",
    "    if not cols_to_standarize:\n",
    "        return df\n",
    "\n",
    "    if norm:\n",
    "        normalizer = Normalizer(norm=norm)\n",
    "        df[cols_to_standarize] = normalizer.fit_transform(df[cols_to_standarize])\n",
    "        return df\n",
    "\n",
    "    if method == 'z-score':\n",
    "        scaler = StandardScaler()\n",
    "    elif method == 'min-max':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == 'robust':\n",
    "        scaler = RobustScaler()\n",
    "    elif method == 'max-abs':\n",
    "        scaler = MaxAbsScaler()\n",
    "    elif method == 'log':\n",
    "        df[cols_to_standarize] = df[cols_to_standarize].apply(lambda x: np.log1p(x))\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method '{method}'. Choose 'z-score', 'min-max', 'robust', 'max-abs', or 'log'.\")\n",
    "    df[cols_to_standarize] = scaler.fit_transform(df[cols_to_standarize])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
